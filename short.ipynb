{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 10>This notebook is a tool to Build a graph for spatio-temporal anomaly detection in 5G networks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`First: we need those import`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import re\n",
    "import Tools_first_step as tfs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threashold is required to select when we want an edge or not using this equation:\n",
    "\n",
    "$$   \n",
    "    A_{i,j}= \n",
    "    \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    0 & \\text{if  ~ } S_{i,j} \\leq T \\\\\n",
    "    1 & \\text{if ~} S_{i,j} > T \n",
    "    \\end{array}\n",
    "    \\right.\n",
    "     $$\n",
    "\n",
    "The L is used in a Lambda model in the equation: $$  S_{i,j}=\\lambda |Corr(x_i,x_j)| + (1-\\lambda)|Cosim(x_i, x_j)| $$ (ref. in the article XXX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "threashold=0.90\n",
    "L=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the path of file we need in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset='/dataset_end_total_preprocessed.csv'\n",
    "doc_path=\"truc.txt\"\n",
    "matrix_correlation_path='corr_matrice.csv' #not mandatory, we can also compute directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## I) Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(path_dataset, nrows=2) \n",
    "#nrows can be used or not\n",
    "#depending if we need to calculate the correlation matrix or import it\n",
    "columns=df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify the dataset :\n",
    "print(df.shape)\n",
    "col_to_keep=tfs.rmv_Unnamed(df.columns)\n",
    "df=df[col_to_keep]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) This part will generate a graph with our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### For this, we need to tokenize the feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "name_columns=df.columns\n",
    "print(tfs.estimate_total_number_of_word(name_columns))\n",
    "number_complet=tfs.estimate_total_number_of_word(name_columns)\n",
    "print(tfs.find_frequence_of_words(name_columns)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tfs.split_all(name_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dico_all_word=tfs.find_frequence_of_words(name_columns)[1]\n",
    "dico2=tfs.find_frequence_of_words(name_columns)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "new_dico_all_word=tfs.cleaning_txt_documentation(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to get every word, including those that are only in the dataset, so we just add the column name at the end of the description file to be sure to have it\n",
    "final_dictionnary=new_dico_all_word+tfs.split_all(name_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Using word2vec, we transform our token, then features names into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we will probably have to try BERT with the time ! BERT seams better today, but we have to give context for this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load pretrained model (download if needed: GoogleNews-vectors-negative300.bin)\n",
    "pretrained_model = KeyedVectors.load_word2vec_format('/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "model = Word2Vec(vector_size=300, min_count=1)\n",
    "model.build_vocab([list(pretrained_model.key_to_index.keys())], update=False)\n",
    "model.wv.vectors = pretrained_model.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(final_dictionnary, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(final_dictionnary, total_examples=len(final_dictionnary), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "a=tfs.number_a_word_in_doc(name_columns, dico2, dico_all_word)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "score_for_each=[]\n",
    "for word, freq in dico2.items():\n",
    "    a=tfs.TF_new_version(word, dico2, number_complet)\n",
    "    score_for_each.append(a)\n",
    "\n",
    "plt.plot(score_for_each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Now we need to use a similarity score to evaluate the difference between two features names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "sequence1 = model.wv['scaling']\n",
    "sequence2 = model.wv['collector']\n",
    "similarity = tfs.cosine_similarity(sequence1, sequence2)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "a,b=tfs.score_similarity_current(name_columns, model)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[('node_cpu_scaling_frequency_hertz{cpu=\"0\"}_server_1','node_cpu_scaling_frequency_hertz{cpu=\"5\"}_server_1')])#'process_open_fds_server_6')])#)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "nodes=name_columns\n",
    "\n",
    "#cosine_similarities, vectores_places=tfs.score_similarity_current(name_columns, model_light)\n",
    "cosine_similarities_25d, vectores_places_25d=tfs.score_similarity_current(name_columns, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Once this is done, we can plot the first graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#tfs.plot_graph_v1(cosine_similarities,threashold,vectores_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#xe,ye,ze, all_edges=tfs.get_edges_v3(cosine_similarities,threashold, vectores_places, \"server\")\n",
    "#print(all_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Tools_first_step as tfs\n",
    "xe,ye,ze, all_edges=tfs.get_edges_topK(cosine_similarities_25d,threashold, vectores_places_25d, \"server\", topK=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#print(all_edges[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#medium_node_number=tfs.get_medium_number_of_edges(all_edges)\n",
    "#print(medium_node_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#nb=tfs.get_distribution_number_of_edges_per_nodes(\"server\",cosine_similarities,threashold, vectores_places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### We can even plot part of the graph that contains a specific word (in the dictionnary of word in all the features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#tfs.plot_only_part_data(cosine_similarities,threashold,vectores_places,\"server_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) This part will propose to modify the edges selections, by using also the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#corr_mat=df.corr()\n",
    "\n",
    "#Here we decide to load the correlation instead of compute it\n",
    "\n",
    "corr_mat=np.genfromtxt(matrix_correlation_path, delimiter=',')\n",
    "corr_mat=pd.DataFrame(corr_mat, columns=df.columns)\n",
    "for i in range(corr_mat.shape[0]):\n",
    "    for j in range(i+1, corr_mat.shape[1]):\n",
    "        corr_mat.iloc[j,i]=corr_mat.iloc[i,j]\n",
    "\n",
    "np.fill_diagonal(corr_mat.values, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat.index=df.columns\n",
    "corr_mat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_correlation=tfs.change_into_dico(corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#tfs.print_corr_mat(corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(len(dico_correlation))\n",
    "print(len(cosine_similarities_25d))\n",
    "print(len(corr_mat.iloc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Final score:\n",
    "\n",
    "The function \"Build_new_score_product\" produce the score given by the equation: \n",
    "\n",
    "$$ S_{i,j}= |Corr(x_i,x_j)|*|Cosim(x_i, x_j)| $$\n",
    "\n",
    "The If we prefer the Lambda model from the equation:\n",
    "\n",
    "$$  S_{i,j}=\\lambda |Corr(x_i,x_j)| + (1-\\lambda)|Cosim(x_i, x_j)|$$\n",
    "\n",
    "We can use here instead \"build_score_lambda\"\n",
    "\n",
    "If we do not want to use a TopK function, we can use get_edges_v3  or simply select a k high enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My_dictionary=tfs.build_new_score_product(dico_correlation, cosine_similarities) #no plot in >3D warning\n",
    "My_dictionary_25=tfs.build_new_score_product(dico_correlation, cosine_similarities_25d)\n",
    "#my_edges_test= tfs.edges_dico(My_dictionary, threashold)\n",
    "my_edges_test_25=tfs.edges_dico(My_dictionary_25, threashold)\n",
    "#useless1,useless2,useless2, edges_complet=tfs.get_edges_topK(My_dictionary,threashold,vectores_places,\"server\", 20)\n",
    "useless4,useless5,useless6, edges_complet_25=tfs.get_edges_v3(My_dictionary_25,threashold,vectores_places_25d,\"server\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#tfs.get_distribution_number_of_edges_per_nodes(\"cpu\",My_dictionary, threashold, vectores_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "new_distrib=tfs.get_distribution_number_of_edges_per_nodes(\"server\",My_dictionary_25,threashold, vectores_places_25d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tfs.plot_only_part_data(My_dictionary_25,threashold,vectores_places_25d,\"server_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "useless1,useless2,useless2, edges_complet=tfs.get_edges_v3(My_dictionary_25,threashold,vectores_places_25d,\"cpu\")\n",
    "med2=tfs.get_medium_number_of_edges(edges_complet)\n",
    "print(med2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV) This section build the adjacency matrix that we willl use in GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "adj_25=tfs.build_adjacency_matrix_v2(edges_complet_25[1:],corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "adj_25np=np.array(adj_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#visualize the matrix:\n",
    "downsampled_data = adj_25np[::1, ::1]  # we can downsample if we prefer\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(downsampled_data, cmap='viridis')\n",
    "plt.title('Adjacency matrix 25D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### We made 2 differents models for words, one words projected in 25 dimensions, the other in 3, now we compare the two matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V) Finaly we have to save/load the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the path for saving in the right place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.savetxt('/adj25d_th09.csv',adj_25np, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Here is how to load after\n",
    "t1=np.genfromtxt('/adj25d_th09.csv', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
